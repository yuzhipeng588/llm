{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4LDEYxS0suaCZ+QWHIkGa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuzhipeng588/llm/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datatrove.pipeline.readers import ParquetReader\n",
        "\n",
        "# limit determines how many documents will be streamed (remove for all)\n",
        "# to fetch a specific dump: hf://datasets/HuggingFaceFW/fineweb/data/CC-MAIN-2024-10\n",
        "# replace \"data\" with \"sample/100BT\" to use the 100BT sample\n",
        "data_reader = ParquetReader(\"hf://datasets/HuggingFaceFW/fineweb/data\", limit=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gtCpTNL53cnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "text = '\\n'.join([doc.text for doc in data_reader()])\n",
        "#text = data_reader().__next__().text\n",
        "print(\"Text Length: \", len(text))\n",
        "\n",
        "def get_next_token(tokens: list[int]) -> list[int]:\n",
        "  token_count = collections.defaultdict(int)\n",
        "  for pair in zip(tokens, tokens[1:]):\n",
        "    token_count[pair] = token_count.get(pair, 0) + 1\n",
        "\n",
        "  return max(token_count, key=token_count.get)\n",
        "\n",
        "def merge(tokens: list[int], new_token_pair: tuple, new_token: int) -> list[int]:\n",
        "  new_tokens = []\n",
        "  i = 0\n",
        "  while i < len(tokens):\n",
        "    if tokens[i:i+2] == list(new_token_pair):\n",
        "      new_tokens.append(new_token)\n",
        "      i+=2\n",
        "    else:\n",
        "      new_tokens.append(tokens[i])\n",
        "      i+=1\n",
        "  return new_tokens\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self, text, token_limit):\n",
        "    assert token_limit > 256, \"Token limit must be greater than 256\"\n",
        "    self.token_limit = token_limit\n",
        "    self.vacob = {i: bytes([i]) for i in range(256)}\n",
        "    self.reverse_vacob = {bytes([i]): i for i in range(256)}\n",
        "    tokens = text.encode('utf-8')\n",
        "    tokens = list(map(int, tokens))\n",
        "    while token_limit > len(self.vacob):\n",
        "      new_token_pair = get_next_token(tokens)\n",
        "      new_token = len(self.vacob)\n",
        "      new_token_bytes = self.vacob[new_token_pair[0]] + self.vacob[new_token_pair[1]]\n",
        "      self.vacob[new_token] = new_token_bytes\n",
        "      self.reverse_vacob[new_token_bytes] = new_token\n",
        "      tokens = merge(tokens, new_token_pair, new_token)\n",
        "\n",
        "  # Encode the text with the longest tokens.\n",
        "  def encode(self, text):\n",
        "    i = 0\n",
        "    j = 1\n",
        "    tokens = []\n",
        "    encoded_text = text.encode('utf-8')\n",
        "    while i < len(encoded_text) and j < len(encoded_text) + 1:\n",
        "      if encoded_text[i:j] in self.reverse_vacob:\n",
        "        if j - i == 1:\n",
        "          tokens.append(self.reverse_vacob[encoded_text[i:j]])\n",
        "        else:\n",
        "          tokens[-1] = self.reverse_vacob[encoded_text[i:j]]\n",
        "        j+=1\n",
        "      else:\n",
        "        i = j - 1\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return b''.join([self.vacob[token] for token in tokens]).decode('utf-8')\n",
        "\n",
        "tokenizer = Tokenizer(text, token_limit=512)"
      ],
      "metadata": {
        "id": "X4pztV6a4U5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test encoding and decoding\n",
        "assert tokenizer.decode(tokenizer.encode(text)) == text"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_AOoLRsU8RcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}