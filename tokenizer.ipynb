{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlximtXSarPX/0NIFnZQFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuzhipeng588/llm/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datatrove.pipeline.readers import ParquetReader\n",
        "\n",
        "# limit determines how many documents will be streamed (remove for all)\n",
        "# to fetch a specific dump: hf://datasets/HuggingFaceFW/fineweb/data/CC-MAIN-2024-10\n",
        "# replace \"data\" with \"sample/100BT\" to use the 100BT sample\n",
        "data_reader = ParquetReader(\"hf://datasets/HuggingFaceFW/fineweb/data\", limit=100)\n",
        "\n",
        "\n",
        "import collections\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "text = '\\n'.join([doc.text for doc in data_reader()])\n",
        "token_limit = 512\n",
        "#text = data_reader().__next__().text\n",
        "print(\"Text Length: \", len(text))\n",
        "\n",
        "def get_next_token(tokens: list[int]) -> list[int]:\n",
        "  token_count = collections.defaultdict(int)\n",
        "  for pair in zip(tokens, tokens[1:]):\n",
        "    token_count[pair] = token_count.get(pair, 0) + 1\n",
        "\n",
        "  return max(token_count, key=token_count.get)\n",
        "\n",
        "def merge(tokens: list[int], new_token_pair: tuple, new_token: int) -> list[int]:\n",
        "  new_tokens = []\n",
        "  i = 0\n",
        "  while i < len(tokens):\n",
        "    if tokens[i:i+2] == list(new_token_pair):\n",
        "      new_tokens.append(new_token)\n",
        "      i+=2\n",
        "    else:\n",
        "      new_tokens.append(tokens[i])\n",
        "      i+=1\n",
        "  return new_tokens\n",
        "\n",
        "class SimpleUtf8Tokenizer(PreTrainedTokenizer):\n",
        "  def __init__(self, vocab_file=None,\n",
        "        merges_file=None,\n",
        "        vocab=None,\n",
        "        merges=None, **kwargs):\n",
        "\n",
        "    # Handle both loading from files and instantiation from in-memory data\n",
        "    if vocab_file and merges_file:\n",
        "        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.vocab = json.load(f)\n",
        "        with open(merges_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            merges_lines = f.read().splitlines()\n",
        "            self.merges = [tuple(line.split()) for line in merges_lines]\n",
        "    elif vocab and merges:\n",
        "        self.vocab = vocab\n",
        "        self.merges = merges\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You must provide either vocab/merges data or file paths.\"\n",
        "        )\n",
        "    print(self.vocab)\n",
        "    self.id_to_token_str = {v:k for k,v in self.vocab.items()}\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  @classmethod\n",
        "  def train(cls, text: str, vocab_size: int, **kwargs):\n",
        "    try:\n",
        "      tokens = text.encode('utf-8')\n",
        "      tokens = list(map(int, tokens))\n",
        "    except UnicodeEncodeError:\n",
        "      raise ValueError(\"Text must be encoded in UTF-8\")\n",
        "\n",
        "    special_tokens = set()\n",
        "    if 'unk_token' in kwargs:\n",
        "      special_tokens.add(kwargs['unk_token'])\n",
        "\n",
        "    assert vocab_size > 256 + len(special_tokens), \"Token limit must be greater than 256 + special tokens\"\n",
        "    bytes_vocab = {i: bytes([i]) for i in range(256)}\n",
        "    for token in special_tokens:\n",
        "      bytes_vocab[len(bytes_vocab)] = token.encode('utf-8')\n",
        "    reverse_bytes_vocab = {v: k for k, v in bytes_vocab.items()}\n",
        "    merges = []\n",
        "    while vocab_size > len(bytes_vocab):\n",
        "      new_token_pair = get_next_token(tokens)\n",
        "      new_token = len(bytes_vocab)\n",
        "      new_token_bytes = bytes_vocab[new_token_pair[0]] + bytes_vocab[new_token_pair[1]]\n",
        "\n",
        "      bytes_vocab[new_token] = new_token_bytes\n",
        "      reverse_bytes_vocab[new_token_bytes] = new_token\n",
        "      tokens = merge(tokens, new_token_pair, new_token)\n",
        "      merges.append((bytes_vocab[new_token_pair[0]], bytes_vocab[new_token_pair[1]]))\n",
        "\n",
        "    str_vocab = {v.decode('latin-1'): k for k, v in bytes_vocab.items()}\n",
        "    str_merges = [(k.decode('latin-1'), v.decode('latin-1')) for k, v in merges]\n",
        "    return cls(vocab=str_vocab, merges=str_merges, **kwargs)\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self) -> int:\n",
        "      # The vocabulary consists of all 256 possible bytes.\n",
        "      return len(self.vocab)\n",
        "\n",
        "  def get_vocab(self) -> Dict[str, int]:\n",
        "      \"\"\"\n",
        "      Returns the vocabulary as a dictionary of strings to integers.\n",
        "      \"\"\"\n",
        "      # Create a mapping from the string representation of each byte to its integer value.\n",
        "      return self.vocab\n",
        "\n",
        "  def _convert_token_to_id(self, token: str) -> int:\n",
        "      \"\"\"\n",
        "      Converts a token (a single-byte string) into its integer byte value.\n",
        "      \"\"\"\n",
        "      # The token is a single character, and its ord() value is its byte value.\n",
        "      return self.vocab[token]\n",
        "\n",
        "  def _convert_id_to_token(self, index: int) -> str:\n",
        "      \"\"\"\n",
        "      Converts an integer byte value into its single-byte string representation.\n",
        "      \"\"\"\n",
        "      # Convert the integer to its character representation using latin-1.\n",
        "      return self.id_to_token_str[index]\n",
        "\n",
        "  def save_vocabulary(\n",
        "      self, save_directory: str, filename_prefix: str | None = None\n",
        "  ) -> Tuple[str]:\n",
        "      \"\"\"Saves the vocabulary and merges to files.\"\"\"\n",
        "      if not os.path.isdir(save_directory):\n",
        "          os.makedirs(save_directory)\n",
        "\n",
        "      # Save the vocabulary file\n",
        "      vocab_file = os.path.join(\n",
        "          save_directory, (filename_prefix or \"\") + \"vocab.json\"\n",
        "      )\n",
        "      with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "          json.dump(self.vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "      # Save the merges file\n",
        "      merges_file = os.path.join(\n",
        "          save_directory, (filename_prefix or \"\") + \"merges.txt\"\n",
        "      )\n",
        "      with open(merges_file, \"w\", encoding=\"utf-8\") as f:\n",
        "          for p1, p2 in self.merges:\n",
        "              f.write(f\"{p1} {p2}\\n\")\n",
        "\n",
        "      return (vocab_file, merges_file)\n",
        "\n",
        "  # Encode the text with the longest tokens.\n",
        "  def encode(self, text):\n",
        "    try:\n",
        "      encoded_text = text.encode('utf-8')\n",
        "    except UnicodeEncodeError:\n",
        "      raise ValueError(\"Text must be encoded in UTF-8\")\n",
        "\n",
        "    i = 0\n",
        "    j = 1\n",
        "    tokens = []\n",
        "    while i < len(text) and j < len(text) + 1:\n",
        "      if text[i:j] in self.vocab:\n",
        "        if j - i == 1:\n",
        "          tokens.append(self.vocab[text[i:j]])\n",
        "        else:\n",
        "          tokens[-1] = self.vocab[text[i:j]]\n",
        "        j+=1\n",
        "      else:\n",
        "        i = j - 1\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return b''.join([self.id_to_token_str[token].encode('utf8') for token in tokens]).decode('utf-8')\n",
        "\n",
        "AutoTokenizer.register(\"SimpleUtf8Tokenizer\", slow_tokenizer_class=SimpleUtf8Tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4pztV6a4U5x",
        "outputId": "acdd564b-e798-41fb-9833-8bde9f02fa63"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-09-08 01:12:50.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file CC-MAIN-2013-20/000_00000.parquet, 1/27468\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Length:  252642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleUtf8Tokenizer.train(text, token_limit, unk_token=\"<unk>\")\n",
        "\n",
        "\n",
        "save_directory = \"./simple_utf8_tokenizer\"\n",
        "print(f\"Saving custom tokenizer to '{save_directory}'...\")\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "print(\"Tokenizer saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mybH-jCyNPMe",
        "outputId": "836f11f9-c2c4-411b-814e-4aadeb4a675c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\x00': 0, '\\x01': 1, '\\x02': 2, '\\x03': 3, '\\x04': 4, '\\x05': 5, '\\x06': 6, '\\x07': 7, '\\x08': 8, '\\t': 9, '\\n': 10, '\\x0b': 11, '\\x0c': 12, '\\r': 13, '\\x0e': 14, '\\x0f': 15, '\\x10': 16, '\\x11': 17, '\\x12': 18, '\\x13': 19, '\\x14': 20, '\\x15': 21, '\\x16': 22, '\\x17': 23, '\\x18': 24, '\\x19': 25, '\\x1a': 26, '\\x1b': 27, '\\x1c': 28, '\\x1d': 29, '\\x1e': 30, '\\x1f': 31, ' ': 32, '!': 33, '\"': 34, '#': 35, '$': 36, '%': 37, '&': 38, \"'\": 39, '(': 40, ')': 41, '*': 42, '+': 43, ',': 44, '-': 45, '.': 46, '/': 47, '0': 48, '1': 49, '2': 50, '3': 51, '4': 52, '5': 53, '6': 54, '7': 55, '8': 56, '9': 57, ':': 58, ';': 59, '<': 60, '=': 61, '>': 62, '?': 63, '@': 64, 'A': 65, 'B': 66, 'C': 67, 'D': 68, 'E': 69, 'F': 70, 'G': 71, 'H': 72, 'I': 73, 'J': 74, 'K': 75, 'L': 76, 'M': 77, 'N': 78, 'O': 79, 'P': 80, 'Q': 81, 'R': 82, 'S': 83, 'T': 84, 'U': 85, 'V': 86, 'W': 87, 'X': 88, 'Y': 89, 'Z': 90, '[': 91, '\\\\': 92, ']': 93, '^': 94, '_': 95, '`': 96, 'a': 97, 'b': 98, 'c': 99, 'd': 100, 'e': 101, 'f': 102, 'g': 103, 'h': 104, 'i': 105, 'j': 106, 'k': 107, 'l': 108, 'm': 109, 'n': 110, 'o': 111, 'p': 112, 'q': 113, 'r': 114, 's': 115, 't': 116, 'u': 117, 'v': 118, 'w': 119, 'x': 120, 'y': 121, 'z': 122, '{': 123, '|': 124, '}': 125, '~': 126, '\\x7f': 127, '\\x80': 128, '\\x81': 129, '\\x82': 130, '\\x83': 131, '\\x84': 132, '\\x85': 133, '\\x86': 134, '\\x87': 135, '\\x88': 136, '\\x89': 137, '\\x8a': 138, '\\x8b': 139, '\\x8c': 140, '\\x8d': 141, '\\x8e': 142, '\\x8f': 143, '\\x90': 144, '\\x91': 145, '\\x92': 146, '\\x93': 147, '\\x94': 148, '\\x95': 149, '\\x96': 150, '\\x97': 151, '\\x98': 152, '\\x99': 153, '\\x9a': 154, '\\x9b': 155, '\\x9c': 156, '\\x9d': 157, '\\x9e': 158, '\\x9f': 159, '\\xa0': 160, '¡': 161, '¢': 162, '£': 163, '¤': 164, '¥': 165, '¦': 166, '§': 167, '¨': 168, '©': 169, 'ª': 170, '«': 171, '¬': 172, '\\xad': 173, '®': 174, '¯': 175, '°': 176, '±': 177, '²': 178, '³': 179, '´': 180, 'µ': 181, '¶': 182, '·': 183, '¸': 184, '¹': 185, 'º': 186, '»': 187, '¼': 188, '½': 189, '¾': 190, '¿': 191, 'À': 192, 'Á': 193, 'Â': 194, 'Ã': 195, 'Ä': 196, 'Å': 197, 'Æ': 198, 'Ç': 199, 'È': 200, 'É': 201, 'Ê': 202, 'Ë': 203, 'Ì': 204, 'Í': 205, 'Î': 206, 'Ï': 207, 'Ð': 208, 'Ñ': 209, 'Ò': 210, 'Ó': 211, 'Ô': 212, 'Õ': 213, 'Ö': 214, '×': 215, 'Ø': 216, 'Ù': 217, 'Ú': 218, 'Û': 219, 'Ü': 220, 'Ý': 221, 'Þ': 222, 'ß': 223, 'à': 224, 'á': 225, 'â': 226, 'ã': 227, 'ä': 228, 'å': 229, 'æ': 230, 'ç': 231, 'è': 232, 'é': 233, 'ê': 234, 'ë': 235, 'ì': 236, 'í': 237, 'î': 238, 'ï': 239, 'ð': 240, 'ñ': 241, 'ò': 242, 'ó': 243, 'ô': 244, 'õ': 245, 'ö': 246, '÷': 247, 'ø': 248, 'ù': 249, 'ú': 250, 'û': 251, 'ü': 252, 'ý': 253, 'þ': 254, 'ÿ': 255, '<unk>': 256, 'e ': 257, 's ': 258, 'th': 259, 'in': 260, 't ': 261, 'd ': 262, 'er': 263, 'an': 264, 'on': 265, 'en': 266, ' th': 267, 'or': 268, 'y ': 269, ', ': 270, 'ou': 271, 'ar': 272, 'al': 273, 'o ': 274, 'ti': 275, 're': 276, ' the ': 277, 'ing': 278, 'of': 279, '. ': 280, 'and ': 281, 'a ': 282, 'ed ': 283, 'om': 284, 'es': 285, 'to ': 286, 'ic': 287, 'ing ': 288, 'at': 289, 'is ': 290, 'st': 291, 'er ': 292, 'it': 293, 'of ': 294, '.\\n': 295, 'at ': 296, 'es ': 297, 'on ': 298, 'el': 299, 'il': 300, 'or ': 301, 'ch': 302, 'as ': 303, 'in ': 304, 'the ': 305, 'al ': 306, 'is': 307, 'ow': 308, 'as': 309, 'l ': 310, 'le': 311, 'ac': 312, 'ati': 313, 'an ': 314, 'ro': 315, 've ': 316, 'you': 317, 'ec': 318, 'ent': 319, 'am': 320, 'ur': 321, 'wh': 322, 'si': 323, 'â\\x80': 324, 'us': 325, 'ad': 326, 'Th': 327, 'r ': 328, 'ab': 329, 'ol': 330, 'gh': 331, 'em': 332, 'ri': 333, 'th ': 334, 'en ': 335, 'ed': 336, 'ly ': 337, 'for ': 338, 'lo': 339, 'k ': 340, 'ha': 341, 'wi': 342, 'com': 343, 'for': 344, 'li': 345, 'un': 346, 'to': 347, 'ent ': 348, 'ver': 349, 'ag': 350, 'le ': 351, 'it ': 352, 'et': 353, 'id': 354, 'fr': 355, 'su': 356, 'ne': 357, 'sh': 358, 'are ': 359, 'pro': 360, 'op': 361, 'os': 362, 'ter': 363, 'ay': 364, 'ex': 365, 'ir': 366, 'con': 367, 'be ': 368, 'â\\x80\\x99': 369, 'ere ': 370, 'tr': 371, 'you ': 372, 'ul': 373, 'out ': 374, 'of the ': 375, 'im': 376, 'n ': 377, 's, ': 378, 'di': 379, 'I ': 380, 'from': 381, 'ear': 382, 'ak': 383, 'p ': 384, '- ': 385, 'that ': 386, 'ce ': 387, 'e, ': 388, 'oun': 389, 'with ': 390, 'ation': 391, 'per': 392, 'in the ': 393, 'ay ': 394, 'res': 395, '. Th': 396, 'ev': 397, 'was ': 398, \"'s \": 399, 'be': 400, 'have ': 401, 'qu': 402, 'ap': 403, 'no': 404, 'oo': 405, 'bu': 406, 'se': 407, 'ation ': 408, 'pl': 409, 'tic': 410, 'tion': 411, 'all': 412, 'ill ': 413, 'and': 414, 'h ': 415, 'ate ': 416, 'ei': 417, 's and ': 418, 'ow ': 419, 'ch ': 420, 'w ': 421, 'ey ': 422, ' that ': 423, 'off': 424, 'uc': 425, 'from ': 426, 'your ': 427, 'par': 428, 'will ': 429, 'wor': 430, ': ': 431, 'ts ': 432, 'one ': 433, 'â\\x80\\x99s ': 434, 'ld ': 435, 'po': 436, 'ers ': 437, 'y, ': 438, 'ough': 439, 'all ': 440, 'ff': 441, 'ity ': 442, 'has ': 443, 'et ': 444, 'de': 445, '00': 446, 'um': 447, 'ic ': 448, 'g ': 449, 'we': 450, 'oc': 451, 'ain': 452, 'cl': 453, 'not ': 454, 'Ch': 455, 'f ': 456, 'ang': 457, 'mor': 458, 'ould ': 459, 'sp': 460, 'to the ': 461, ', the ': 462, '. I': 463, 'igh': 464, 'so ': 465, 'by ': 466, 'tion ': 467, 'ted ': 468, 'vi': 469, '. A': 470, 'fe': 471, 'pr': 472, 'Wh': 473, 'gr': 474, 'about ': 475, 'his ': 476, 'on the ': 477, '\\n- ': 478, 'comm': 479, 'rec': 480, 'eir ': 481, ' this ': 482, 'end': 483, 'ther ': 484, 'enc': 485, 'te': 486, 'por': 487, ' and ': 488, 'can ': 489, 'ra': 490, 'ally ': 491, 'any ': 492, 'ich ': 493, 'our': 494, 'tim': 495, 'ome ': 496, 'ents ': 497, 'ar ': 498, 'ast ': 499, 'In': 500, 'mo': 501, 'go': 502, 'year': 503, '\" ': 504, 'est ': 505, 'sid': 506, 'e.\\n': 507, 'but ': 508, 's of ': 509, 'The ': 510, 'ter ': 511}\n",
            "Saving custom tokenizer to './simple_utf8_tokenizer'...\n",
            "Tokenizer saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "print(\"\\nLoading tokenizer back...\")\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    save_directory, trust_remote_code=True, vocab_file=save_directory + \"/vocab.json\", merges_file=save_directory + \"/merges.txt\"\n",
        ")\n",
        "\n",
        "# Verify it works\n",
        "text = \"hello custom\"\n",
        "encoded = loaded_tokenizer.encode(text)\n",
        "print(f\"\\nOriginal text: '{text}'\")\n",
        "print(f\"Encoded with loaded tokenizer: {encoded}\")\n",
        "decoded = loaded_tokenizer.decode(encoded)\n",
        "print(f\"Decoded text: '{decoded}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1cP4vFbjGgs",
        "outputId": "a1e484de-6587-42da-d7c0-1f6446102839"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading tokenizer back...\n",
            "{'\\x00': 0, '\\x01': 1, '\\x02': 2, '\\x03': 3, '\\x04': 4, '\\x05': 5, '\\x06': 6, '\\x07': 7, '\\x08': 8, '\\t': 9, '\\n': 10, '\\x0b': 11, '\\x0c': 12, '\\r': 13, '\\x0e': 14, '\\x0f': 15, '\\x10': 16, '\\x11': 17, '\\x12': 18, '\\x13': 19, '\\x14': 20, '\\x15': 21, '\\x16': 22, '\\x17': 23, '\\x18': 24, '\\x19': 25, '\\x1a': 26, '\\x1b': 27, '\\x1c': 28, '\\x1d': 29, '\\x1e': 30, '\\x1f': 31, ' ': 32, '!': 33, '\"': 34, '#': 35, '$': 36, '%': 37, '&': 38, \"'\": 39, '(': 40, ')': 41, '*': 42, '+': 43, ',': 44, '-': 45, '.': 46, '/': 47, '0': 48, '1': 49, '2': 50, '3': 51, '4': 52, '5': 53, '6': 54, '7': 55, '8': 56, '9': 57, ':': 58, ';': 59, '<': 60, '=': 61, '>': 62, '?': 63, '@': 64, 'A': 65, 'B': 66, 'C': 67, 'D': 68, 'E': 69, 'F': 70, 'G': 71, 'H': 72, 'I': 73, 'J': 74, 'K': 75, 'L': 76, 'M': 77, 'N': 78, 'O': 79, 'P': 80, 'Q': 81, 'R': 82, 'S': 83, 'T': 84, 'U': 85, 'V': 86, 'W': 87, 'X': 88, 'Y': 89, 'Z': 90, '[': 91, '\\\\': 92, ']': 93, '^': 94, '_': 95, '`': 96, 'a': 97, 'b': 98, 'c': 99, 'd': 100, 'e': 101, 'f': 102, 'g': 103, 'h': 104, 'i': 105, 'j': 106, 'k': 107, 'l': 108, 'm': 109, 'n': 110, 'o': 111, 'p': 112, 'q': 113, 'r': 114, 's': 115, 't': 116, 'u': 117, 'v': 118, 'w': 119, 'x': 120, 'y': 121, 'z': 122, '{': 123, '|': 124, '}': 125, '~': 126, '\\x7f': 127, '\\x80': 128, '\\x81': 129, '\\x82': 130, '\\x83': 131, '\\x84': 132, '\\x85': 133, '\\x86': 134, '\\x87': 135, '\\x88': 136, '\\x89': 137, '\\x8a': 138, '\\x8b': 139, '\\x8c': 140, '\\x8d': 141, '\\x8e': 142, '\\x8f': 143, '\\x90': 144, '\\x91': 145, '\\x92': 146, '\\x93': 147, '\\x94': 148, '\\x95': 149, '\\x96': 150, '\\x97': 151, '\\x98': 152, '\\x99': 153, '\\x9a': 154, '\\x9b': 155, '\\x9c': 156, '\\x9d': 157, '\\x9e': 158, '\\x9f': 159, '\\xa0': 160, '¡': 161, '¢': 162, '£': 163, '¤': 164, '¥': 165, '¦': 166, '§': 167, '¨': 168, '©': 169, 'ª': 170, '«': 171, '¬': 172, '\\xad': 173, '®': 174, '¯': 175, '°': 176, '±': 177, '²': 178, '³': 179, '´': 180, 'µ': 181, '¶': 182, '·': 183, '¸': 184, '¹': 185, 'º': 186, '»': 187, '¼': 188, '½': 189, '¾': 190, '¿': 191, 'À': 192, 'Á': 193, 'Â': 194, 'Ã': 195, 'Ä': 196, 'Å': 197, 'Æ': 198, 'Ç': 199, 'È': 200, 'É': 201, 'Ê': 202, 'Ë': 203, 'Ì': 204, 'Í': 205, 'Î': 206, 'Ï': 207, 'Ð': 208, 'Ñ': 209, 'Ò': 210, 'Ó': 211, 'Ô': 212, 'Õ': 213, 'Ö': 214, '×': 215, 'Ø': 216, 'Ù': 217, 'Ú': 218, 'Û': 219, 'Ü': 220, 'Ý': 221, 'Þ': 222, 'ß': 223, 'à': 224, 'á': 225, 'â': 226, 'ã': 227, 'ä': 228, 'å': 229, 'æ': 230, 'ç': 231, 'è': 232, 'é': 233, 'ê': 234, 'ë': 235, 'ì': 236, 'í': 237, 'î': 238, 'ï': 239, 'ð': 240, 'ñ': 241, 'ò': 242, 'ó': 243, 'ô': 244, 'õ': 245, 'ö': 246, '÷': 247, 'ø': 248, 'ù': 249, 'ú': 250, 'û': 251, 'ü': 252, 'ý': 253, 'þ': 254, 'ÿ': 255, '<unk>': 256, 'e ': 257, 's ': 258, 'th': 259, 'in': 260, 't ': 261, 'd ': 262, 'er': 263, 'an': 264, 'on': 265, 'en': 266, ' th': 267, 'or': 268, 'y ': 269, ', ': 270, 'ou': 271, 'ar': 272, 'al': 273, 'o ': 274, 'ti': 275, 're': 276, ' the ': 277, 'ing': 278, 'of': 279, '. ': 280, 'and ': 281, 'a ': 282, 'ed ': 283, 'om': 284, 'es': 285, 'to ': 286, 'ic': 287, 'ing ': 288, 'at': 289, 'is ': 290, 'st': 291, 'er ': 292, 'it': 293, 'of ': 294, '.\\n': 295, 'at ': 296, 'es ': 297, 'on ': 298, 'el': 299, 'il': 300, 'or ': 301, 'ch': 302, 'as ': 303, 'in ': 304, 'the ': 305, 'al ': 306, 'is': 307, 'ow': 308, 'as': 309, 'l ': 310, 'le': 311, 'ac': 312, 'ati': 313, 'an ': 314, 'ro': 315, 've ': 316, 'you': 317, 'ec': 318, 'ent': 319, 'am': 320, 'ur': 321, 'wh': 322, 'si': 323, 'â\\x80': 324, 'us': 325, 'ad': 326, 'Th': 327, 'r ': 328, 'ab': 329, 'ol': 330, 'gh': 331, 'em': 332, 'ri': 333, 'th ': 334, 'en ': 335, 'ed': 336, 'ly ': 337, 'for ': 338, 'lo': 339, 'k ': 340, 'ha': 341, 'wi': 342, 'com': 343, 'for': 344, 'li': 345, 'un': 346, 'to': 347, 'ent ': 348, 'ver': 349, 'ag': 350, 'le ': 351, 'it ': 352, 'et': 353, 'id': 354, 'fr': 355, 'su': 356, 'ne': 357, 'sh': 358, 'are ': 359, 'pro': 360, 'op': 361, 'os': 362, 'ter': 363, 'ay': 364, 'ex': 365, 'ir': 366, 'con': 367, 'be ': 368, 'â\\x80\\x99': 369, 'ere ': 370, 'tr': 371, 'you ': 372, 'ul': 373, 'out ': 374, 'of the ': 375, 'im': 376, 'n ': 377, 's, ': 378, 'di': 379, 'I ': 380, 'from': 381, 'ear': 382, 'ak': 383, 'p ': 384, '- ': 385, 'that ': 386, 'ce ': 387, 'e, ': 388, 'oun': 389, 'with ': 390, 'ation': 391, 'per': 392, 'in the ': 393, 'ay ': 394, 'res': 395, '. Th': 396, 'ev': 397, 'was ': 398, \"'s \": 399, 'be': 400, 'have ': 401, 'qu': 402, 'ap': 403, 'no': 404, 'oo': 405, 'bu': 406, 'se': 407, 'ation ': 408, 'pl': 409, 'tic': 410, 'tion': 411, 'all': 412, 'ill ': 413, 'and': 414, 'h ': 415, 'ate ': 416, 'ei': 417, 's and ': 418, 'ow ': 419, 'ch ': 420, 'w ': 421, 'ey ': 422, ' that ': 423, 'off': 424, 'uc': 425, 'from ': 426, 'your ': 427, 'par': 428, 'will ': 429, 'wor': 430, ': ': 431, 'ts ': 432, 'one ': 433, 'â\\x80\\x99s ': 434, 'ld ': 435, 'po': 436, 'ers ': 437, 'y, ': 438, 'ough': 439, 'all ': 440, 'ff': 441, 'ity ': 442, 'has ': 443, 'et ': 444, 'de': 445, '00': 446, 'um': 447, 'ic ': 448, 'g ': 449, 'we': 450, 'oc': 451, 'ain': 452, 'cl': 453, 'not ': 454, 'Ch': 455, 'f ': 456, 'ang': 457, 'mor': 458, 'ould ': 459, 'sp': 460, 'to the ': 461, ', the ': 462, '. I': 463, 'igh': 464, 'so ': 465, 'by ': 466, 'tion ': 467, 'ted ': 468, 'vi': 469, '. A': 470, 'fe': 471, 'pr': 472, 'Wh': 473, 'gr': 474, 'about ': 475, 'his ': 476, 'on the ': 477, '\\n- ': 478, 'comm': 479, 'rec': 480, 'eir ': 481, ' this ': 482, 'end': 483, 'ther ': 484, 'enc': 485, 'te': 486, 'por': 487, ' and ': 488, 'can ': 489, 'ra': 490, 'ally ': 491, 'any ': 492, 'ich ': 493, 'our': 494, 'tim': 495, 'ome ': 496, 'ents ': 497, 'ar ': 498, 'ast ': 499, 'In': 500, 'mo': 501, 'go': 502, 'year': 503, '\" ': 504, 'est ': 505, 'sid': 506, 'e.\\n': 507, 'but ': 508, 's of ': 509, 'The ': 510, 'ter ': 511}\n",
            "\n",
            "Original text: 'hello custom'\n",
            "Encoded with loaded tokenizer: [104, 299, 339, 32, 99, 325, 347, 109]\n",
            "Decoded text: 'hello custom'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test encoding and decoding\n",
        "assert tokenizer.decode(tokenizer.encode(text)) == text"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_AOoLRsU8RcJ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(b'\\x80'.decode('latin-1'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np6QU1WbTUtj",
        "outputId": "ce6632e9-9d9b-490f-bd0a-c9420fd4592b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(bytes([1,3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C9eoTGUphc8",
        "outputId": "a406b17f-1baa-44a4-9f0e-1e750f5b361c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bytes"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}